{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import keras.backend as K\n",
    "\n",
    "IMAGE_SHAPE = [94,24]\n",
    "CHARS = \"ABCDEFGHIJKLMNPQRSTUVWXYZ0123456789\" # exclude I, O\n",
    "CHARS_DICT = {char:i for i, char in enumerate(CHARS)}\n",
    "DECODE_DICT = {i:char for i, char in enumerate(CHARS)}\n",
    "NUM_CLASS = len(CHARS)+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CTCLoss(y_true, y_pred):\n",
    "    # Compute the training-time loss value\n",
    "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class small_basic_block(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,out_channels,name=None,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        out_div4=int(out_channels/4)\n",
    "        self.main_layers = [\n",
    "            keras.layers.Conv2D(out_div4,(1,1),padding='same',kernel_initializer=keras.initializers.glorot_uniform(),bias_initializer=keras.initializers.constant()),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.ReLU(),\n",
    "            keras.layers.Conv2D(out_div4,(3,1),padding='same',kernel_initializer=keras.initializers.glorot_uniform(),bias_initializer=keras.initializers.constant()),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.ReLU(),\n",
    "            keras.layers.Conv2D(out_div4,(1,3),padding='same',kernel_initializer=keras.initializers.glorot_uniform(),bias_initializer=keras.initializers.constant()),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.ReLU(),\n",
    "            keras.layers.Conv2D(out_channels,(1,1),padding='same',kernel_initializer=keras.initializers.glorot_uniform(),bias_initializer=keras.initializers.constant()),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.ReLU(),\n",
    "        ]  \n",
    "    \n",
    "    def call(self,input):\n",
    "        x = input\n",
    "        for layer in self.main_layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test this later\n",
    "\n",
    "class global_context(keras.layers.Layer):\n",
    "    def __init__(self,kernel_size,stride,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ksize = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "    def call(self, input):\n",
    "        x = input \n",
    "        avg_pool = keras.layers.AveragePooling2D(pool_size=self.ksize,strides=self.stride,padding='same')(x)\n",
    "        sq = keras.layers.Lambda(lambda x: tf.math.square(x))(avg_pool)\n",
    "        sqm = keras.layers.Lambda(lambda x: tf.math.reduce_mean(x))(sq)\n",
    "        out = keras.layers.Lambda(lambda x: tf.math.divide(x[0], x[1]))([avg_pool , sqm])\n",
    "        #out = keras.layers.Lambda(lambda x: K.l2_normalize(x,axis=1))(avg_pool)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LPRnet(keras.Model):\n",
    "    def __init__(self, input_shape=(24,94,3), **kwargs):\n",
    "        super(LPRnet, self).__init__(**kwargs)\n",
    "        self.input_layer = keras.layers.Input(input_shape)\n",
    "        self.cnn_layers= [\n",
    "            keras.layers.Conv2D(64,kernel_size = (3,3),strides=1,padding='same',name='main_conv1',kernel_initializer=keras.initializers.glorot_uniform(),bias_initializer=keras.initializers.constant()),\n",
    "            keras.layers.BatchNormalization(name='BN1'),\n",
    "            keras.layers.ReLU(name='RELU1'),\n",
    "            keras.layers.MaxPool2D(pool_size=(3,3),strides=(1,1),name='maxpool2d_1',padding='same'),\n",
    "            small_basic_block(128),\n",
    "            keras.layers.MaxPool2D(pool_size=(3,3),strides=(1,2),name='maxpool2d_2',padding='same'),\n",
    "            small_basic_block(256),\n",
    "            small_basic_block(256),\n",
    "            keras.layers.MaxPool2D(pool_size=(3,3),strides=(1,2),name='maxpool2d_3',padding='same'),\n",
    "            keras.layers.Dropout(0.5),\n",
    "            keras.layers.Conv2D(256,(4,1),strides=1,padding='same',name='main_conv2',kernel_initializer=keras.initializers.glorot_uniform(),bias_initializer=keras.initializers.constant()),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.ReLU(),\n",
    "            keras.layers.Dropout(0.5),\n",
    "            keras.layers.Conv2D(NUM_CLASS,(1,13),padding='same',name='main_conv3',kernel_initializer=keras.initializers.glorot_uniform(),bias_initializer=keras.initializers.constant()),  \n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.ReLU(),\n",
    "        ]\n",
    "        self.out_layers = [\n",
    "            keras.layers.Conv2D(NUM_CLASS,kernel_size=(1,1),strides=(1,1),padding='same',name='conv_out',kernel_initializer=keras.initializers.glorot_uniform(),bias_initializer=keras.initializers.constant()),\n",
    "            #keras.layers.BatchNormalization(),\n",
    "            #keras.layers.ReLU(),\n",
    "        ]\n",
    "        #self.conv_out = keras.layers.Conv2D(NUM_CLASS,kernel_size=(1,1),strides=(1,1),padding='same',name='conv_out')\n",
    "        self.out = self.call(self.input_layer)\n",
    "        super(LPRnet, self).__init__(\n",
    "            inputs=self.input_layer,\n",
    "            outputs=self.out,\n",
    "            **kwargs)\n",
    "\n",
    "    def call(self,inputs,training=False):\n",
    "        x = inputs\n",
    "        layer_outputs = []\n",
    "        for layer in self.cnn_layers:\n",
    "            x = layer(x)\n",
    "            layer_outputs.append(x)\n",
    "        scale1 = global_context((1,4),(1,4))(layer_outputs[0])\n",
    "        scale2 = global_context((1,4),(1,4))(layer_outputs[4])\n",
    "        scale3 = global_context((1,2),(1,2))(layer_outputs[6])\n",
    "        scale5 = global_context((1,2),(1,2))(layer_outputs[7])\n",
    "        #scale4 = keras.layers.Lambda(lambda x: K.l2_normalize(x,axis=1))(x)\n",
    "        sq = keras.layers.Lambda(lambda x: tf.math.square(x))(x)\n",
    "        sqm = keras.layers.Lambda(lambda x: tf.math.reduce_mean(x))(sq)\n",
    "        scale4 = keras.layers.Lambda(lambda x: tf.math.divide(x[0], x[1]))([x , sqm])\n",
    "        gc_concat = keras.layers.Lambda(lambda x: tf.concat([x[0], x[1], x[2], x[3], x[4]],3))([scale1, scale2, scale3, scale5,scale4])\n",
    "        for layer in self.out_layers:\n",
    "            gc_concat = layer(gc_concat)\n",
    "        logits = keras.layers.Lambda(lambda x: tf.math.reduce_mean(x[0],axis=1))([gc_concat])\n",
    "        #transposed_logs = keras.layers.Lambda(lambda x: tf.transpose(x,(1,0,2)))(logits)\n",
    "        logits = keras.layers.Softmax()(logits)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LPRnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=2000,\n",
    "    decay_rate=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),loss =CTCLoss)\n",
    "for layer in model.layers:\n",
    "    print(layer.name,layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "data = []\n",
    "labels = []\n",
    "val_data = []\n",
    "val_labels = []\n",
    "\n",
    "GEN_VAL_PATH = glob.glob('./valid/*.jpg')\n",
    "LP_VAL_PATH = glob.glob('C:\\\\Users\\\\carlos\\\\Desktop\\\\cs\\\\ml-sandbox\\\\ANPR\\\\TFODCourse\\\\test frames\\\\filtered_plates\\\\driving_quezoncity.mp4\\\\*.png')\n",
    "\n",
    "#for file in glob.glob('./test/*.jpg'):\n",
    "#    label=file.split('\\\\')[1]\n",
    "#    image = cv2.imread(file,cv2.IMREAD_COLOR)\n",
    "#    image = cv2.resize(image,(94,24))\n",
    "#    data.append(image/256)\n",
    "#    labels.append([CHARS_DICT[i] for i in label.split('_')[0]])\n",
    "for file in LP_VAL_PATH:\n",
    "    label=file.split('\\\\')[-1]\n",
    "    image = cv2.imread(file,cv2.IMREAD_COLOR)\n",
    "    image = cv2.resize(image,(94,24))\n",
    "    val_data.append(image/256)\n",
    "    val_labels.append([CHARS_DICT[i] for i in label.split('_')[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_set = np.array(data,dtype=np.float32)\n",
    "#training_labels = np.array(labels)\n",
    "#ragged = tf.ragged.constant(training_labels).to_tensor()\n",
    "#dataset = tf.data.Dataset.from_tensor_slices((training_set,ragged)).shuffle(640).repeat(100).batch(64)\n",
    "\n",
    "\n",
    "val_training_set = np.array(val_data,dtype=np.float32)\n",
    "val_training_labels = np.array(val_labels)\n",
    "val_ragged = tf.ragged.constant(val_training_labels).to_tensor()\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_training_set,val_ragged)).shuffle(640).repeat(100).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlos\\AppData\\Local\\Temp/ipykernel_21524/3666392512.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  training_labels = np.array(gen_labels)\n"
     ]
    }
   ],
   "source": [
    "from gen_plates_keras import *\n",
    "gen = ImageGenerator()\n",
    "def generate_dataset(num = 100):\n",
    "    data, labels = gen.generate_images(num)\n",
    "    gen_labels = []\n",
    "    for label in labels:\n",
    "        gen_labels.append([CHARS_DICT[i] for i in label.split('_')[0]])\n",
    "    pics =np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    training_set = np.array(pics,dtype=np.float32)\n",
    "    training_labels = np.array(gen_labels)\n",
    "    ragged = tf.ragged.constant(training_labels).to_tensor()\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((training_set,ragged)).shuffle(640).batch(64)\n",
    "    return dataset\n",
    "\n",
    "standalone_dataset = generate_dataset(num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"my-test-project\", entity=\"clsandoval\")\n",
    "wandb.config = {\n",
    "  \"learning_rate\": 0.001,\n",
    "  \"epochs\": 400,\n",
    "  \"batch_size\": 64\n",
    "}\n",
    "\n",
    "def generate_val_set(num=400):\n",
    "        val_data_gen, val_labels_gen = gen.generate_images(num)\n",
    "        val_labels_encoded = []\n",
    "        val_data_encoded = []\n",
    "        for image,label in zip(val_data_gen, val_labels_gen):\n",
    "                val_data_encoded.append(np.expand_dims(image,axis=0))\n",
    "                val_labels_encoded.append([CHARS_DICT[i] for i in label.split('_')[0]])\n",
    "        return val_data_encoded,val_labels_encoded\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standalone_dataset = generate_dataset(num=10000)\n",
    "#val_dataset = generate_dataset(1000)\n",
    "for j in range(100):\n",
    "    dataset = generate_dataset(num=32000)\n",
    "    model.fit(dataset,validation_data=val_dataset,validation_steps=5,epochs=10,steps_per_epoch=50,callbacks=[WandbCallback()])\n",
    "    print(j,flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "real_images = glob.glob('C:\\\\Users\\\\carlos\\\\Desktop\\\\cs\\\\ml-sandbox\\\\ANPR\\\\TFODCourse\\\\test frames\\\\filtered_plates\\\\driving_quezoncity.mp4\\\\*.png')\n",
    "for file in real_images:\n",
    "    image = cv2.imread(file)\n",
    "    test_image = cv2.resize(image,(94,24))/256\n",
    "    test_image = np.expand_dims(test_image,axis=0)\n",
    "    preds = model.predict(test_image) \n",
    "    decoded = tf.keras.backend.ctc_decode(preds,(24,))\n",
    "    for i in np.array(decoded[0]).reshape(24):\n",
    "        if i >-1:\n",
    "            print(DECODE_DICT[i],end='')\n",
    "    print(\" \"+ file.split('\\\\')[-1].split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('keras_lprnet2.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter('keras_lprnet1.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_images = glob.glob('C:\\\\Users\\\\carlos\\\\Desktop\\\\cs\\\\ml-sandbox\\\\ANPR\\\\TFODCourse\\\\test frames\\\\driving_quezoncity.mp4\\\\*.png')\n",
    "for file in real_images:\n",
    "    image = cv2.imread(file)\n",
    "    test_image = cv2.resize(image,(94,24))/256\n",
    "    test_image = np.expand_dims(test_image,axis=0)\n",
    "    test_image = test_image.astype(np.float32)\n",
    "    interpreter.set_tensor(input_details[0]['index'], test_image)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    decoded = keras.backend.ctc_decode(output_data,(24,))\n",
    "    for i in np.array(decoded[0]).reshape(24):\n",
    "        if i >-1:\n",
    "            print(DECODE_DICT[i],end='')\n",
    "    print(\" \"+ file.split('\\\\')[-1].split('_')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83a3ba54cbd1f7dfc2d1ef373eda2962c312d1019df78bd28c6b3b15cf8d1ffd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
